\documentclass[a4paper, 11pt]{article}
\begin{document}
\section{Introduction}

This document describes the principles and structure behind the xia2dpa 
data model. This is necessary because the problem is a complicated one.
The basic principle here is to have a global ``data repository'' which 
is structured enough to have a hierarchy of data but simple enough that
a value of something can be found easily.

This structure will need to have a few basic properties:

\begin{itemize}
\item{Objects have two ``parts'' - an immutable identity and a set of 
defined properties which may be allowed to vary with time.}
\item{Changes to objects should be recorded as updates, with earlier 
instances being kept. An example follows.}
\item{Getting a mutable property for an immutable object will delegate 
the getting to an immutable child.}
\end{itemize}

The example which follows is:

{
\tiny
\begin{verbatim}
d = Dataset('12287_1_E1_001.img') -> Populate identity from the image
                                     headers for this data set. Initialise
                                     metadata about this dataset, and 
                                     record the creation epoch.

d.index() -> Autoindex the data set. This task will be delegated to an
             autoindexer, which will take as an argument the dataset object.
             The results will be recorded in a list of autoindexing
             solutions and the latest instance in this list returned.
             This method will also check for assertions which may be
             relevant, for instance that the lattice is monoclinic.
             This assertion will also have an associated epoch. If ...

d.getCell() -> Is called and the assertion for the lattice is more recent
               than the source object that getCell gets the information from,
               the source object (in this case the indexing solution) needs
               to be reevaluated. This will mean that any further processing
               based on these results may need to be repeated.
\end{verbatim}
}

The upshot of this is that if I autoindex the data set, process and find in 
cell refinement that the refinement breaks (or process in triclinic, and
check the point group) I can assert that the lattice is something different.
The next ``get'' method will then verify that it's information is up-to-date
and if not will KNOW how to make it so.

This is going to get complicated, but is a fascinating way of working. 
It will mean that the knowledge on how to update objects will have to be
delegated to the objects.

This comes back to the overarching idea that the main() routine in this 
could almost look like:

{
\tiny
\begin{verbatim}
processing_results = Dataset('12287_1_E1_001.img').getProcessing_results()

... or almost ...

structure = Model(sequence = 1vpj.pir,
                  phases = Phase(Dataset({frames:['infl_001.img', 
                                                 'lrem_lr_001.img',
                                                 'lrem_001.img'],
                                          id:[(0.9790, fp, fpp),
                                              (1.0002, fp, fpp)]}))
                  ).getStructure()
\end{verbatim}
}

... taking this to the obvious conclusion, the objects would have ONLY get 
methods - everything else would be passed in through the constructor, and all
actions would be implied by the get methods. Note that ``private'' methods 
would be needed in order to implement the result discovery delegation but
this would be relatively doable.

Time to get a second brain fitted then. This is beginning to look a little 
like hard-core C++ programming.

This then means that the whole architecture is almost programmed in a 
functional 
manner\footnote{http://en.wikipedia.org/wiki/Functional\_programming}. 
Cool. It also means that the schema for the objects is
of relatively little interest, though it would be handy to have some 
lightweight objects for handling this kind of information.

Basic idea here is everything works by lazy evaluation - only compute the
result when asked for it, not when you're asked to compute it.

\subsection{Hierarchy}

This principle really works when ideas of hierarchy are included. For instance,
the first pass at processing the first data set (by definition the reference)
will result in one unit cell. This will then be set as the ``globally 
correct'' one until more information is available. When another set at this
wavelength is processed, a weighted value for the unit cell may be derived
from both sets. When combining all of the data, an overall unit cell may be 
computed from all of the available data sets. 

This means, by implication, that each data processing ``run'' will have
an associated local instance of a global data store. When it comes to 
merging or combining data sets, a further data store will be necessary.
Does this mean that each stage is considered as it's own project, resulting
in an idea that the global project data repository, referred to above, is
some kind of weighted average of all of the local data repositories. Think
of this like a tree-code.

\section{Items of Interest}

\subsection{User Input}

At the beginning, all that is known is what the user has passed in on the 
command line. From this a small number of simple things can be derived. 
In the example above, an example is the relationship between datasets and
f values - e.g. wavelength for an image is read from the header. This is
then used to associate different sweeps collected at the same wavelength 
and so on...

So - an object is needed to hold the information passed in by the user.
Frameworks are then needed to communicate this information to the constructors
as described above, perhaps as part of a dictionary which the constructor
can interrogate as necessary.

The following items will be minimally necessary to make this work:

\begin{itemize}
\item{Frames to use, defined by a frame from the set.}
\item{?Correct beam centre.}
\item{?Wavelength, f', f'' values if available.}
\end{itemize}

\subsection{Learning Things}

From the basic information derived from the command line, some extra 
information can be learned. For example:

\begin{itemize}
\item{Lattice, unit cell.}
\item{Spacegroup?}
\item{Resolution.}
\end{itemize}

At any stage any derived information is a hypothesis. There will be degrees
of reliability associated with each of these, and when asked for the most 
recent (by definition most reliable) instance should be returned.

\section{Defined Objects}

\subsection{Object}

The class Object defines some really basic stuff, and should be inherited 
from by all objects. In particular it defines an identity which will enable
sorting by creation epoch. Since all data will be defined at construction 
time, and identities are immutable, this \emph{should} be safe!

Also - added a mutex to this to allow bottom-up implementations of threading.

Also - add a list of output to each object, to allow recording what each
object actually does. When overall output is wanted, each object could print
it's stuff.

\subsection{Sweep}

A sweep object defines a set of continuous frames. This can therefore be
characterized by phi start and end values, oscillation width, exposure
time, distance, wavelength, template and exposure epoch range. The if the epoch
is NULL then the template will be used for sorting, otherwise start epochs
will be most important.

FIXME the frame identification etc. defined in Dataset should probably be
delegated to Sweep, and then contained in Dataset. All of the ``expertise'' 
should then be obtained through this interface.

In fact the constructor for this should include the header reading 
functioality so that we can be sure that the contents of the object
are correct - also simplifies the interface and ensures that delegation
is correct.

Sweeps should really come from a sweep factory, which should be accessed
from the Dataset object - the Dataset can then decide what is appropriate to
do with the sweeps.

=> define a SweepFactory in the Sweep object which will return a list of 
sweeps. However, there is one niggle - I want sweeps to be able to update
themselves just in case more frames have appeared since they were last
used. This means that there needs to be a higher level of sweep owner
to manage all of this - OR - a sweep has to contain subsweeps or something, 
with those being arranged by collection date. Could ``identify'' sweeps by
the first image (since I will assume image numbers always increment during 
collection.) That way the identity will not change, and so they can be picked 
out.

And it also means that a SweepFactory is possible... Finally, assume that 
image headers do not change! They can therefore be cached in the Printheader
class, which is useful because it will really speed things up when there are
multiple reads

FIXME: Add a feature to identify the detector class and mode, e.g. ``ADSC
Quantum 315 2x2 binned'' and also a short code like ``q315-2x2''.

\subsection{Dataset}

\section{Delegation}

Objects like dataset may have well defined methods for performing tasks.
However, something which could be fun would be to delegate this via
\verb|__getattr__| and a module registry to allow any arbitrary object to
have a punt at performing an operation. Would this be safe?

Or is it a better idea to just have:

{
\tiny
\begin{verbatim}
d = Dataset(...)
d.getLatticeInfo() -> delegate via IndexerFactory to get an implementation,
                      then return the result...
\end{verbatim}
}


\section{Interfaces}

\subsection{Thoughts}

Since some programs present more than one interface, is it appropriate to
inherit from base classes which represent these interfaces as well as 
the ``Driver''? That would ensure that when you say class X implements 
indexer you would be certain that it does, because the indexer interface
would be the only way to get to the functionality.

Yes, this is probably a good idea. The only problem is then to ensure that
there are no name clashes between interfaces which might me multiply 
represented, e.g. indexer and integrater for Mosflm \& XDS.

For information, multiple inheritance does work in Python.

An interesting question is how to handle the directory, template information - 
since almost all (or all?) data processing interfaces will need this
information is it also better to inherit from (or decorate?) a Driver to
handle this information ``invisibly''? Probably. Then all programs which 
require diffraction images should use this information as the sole way of
getting to the images. Since this is hidden invisibly by the CommandLine
singleton there shouldn't be any big problems.

\subsection{Frame Processor}

This is an interface which includes all of the information which may be needed
by something which handles diffraction images. This includes:

\begin{itemize}
\item{Beam position.}
\item{Wavelength.}
\item{Distance.}
\item{Template.}
\item{Directory.}
\item{Header information e.g. width, pixel size.}
\end{itemize}

This should be defined as a basic decorator in the same way that the 
CCP4 decorator works. This will give a little extra work for a lot of 
extra benefit. This will further mean that this goes into the xia2core
definition, which makes it more simply available to the DC module.

[FIXME this section is now to go into xia2core documentation]

These would well suit a decorator if it didn't mean that the other
decorators wouldn't work - looks like we're better off simply working by
multiple inheritance and ducking when things go strange.

[UNFIXME this now needs to stay here!]

Added option to initialize the information from a constructor which takes
an image file - this seems to make sense to me. Haven't made that aspect
of the interface public, don't know whether I should.


\subsection{Indexer}

An indexer should take images to index with (either as a list of a block)
perform indexing and provide the results in a useful fashion. The form
of the results should be an orientation matrix, unit cell, lattice and 
an estimate of the mosaic spread. The refined beam position should also
be returned.

Inputs should be the lattice (optionally), unit cell (optionally).

\end{document}
